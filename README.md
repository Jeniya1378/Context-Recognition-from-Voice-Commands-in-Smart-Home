To perform context recognition from short voice conversations, we can convert audio to text and apply natural language processing (NLP) techniques to analyze the textual content. However, the existing text classification systems that apply NLP techniques to extract meaningful information from texts require large training data. In this paper, we propose a novel framework to extract contexts from short-spoken texts requiring smaller training datasets. This framework exploits the power of transfer learning and uses a fully connected neural network aided with SBERT encoding, and an attention mechanism. Our proposed framework has been evaluated using two datasets containing short smart home commands. Evaluation results demonstrate that our model achieves higher accuracy in context recognition with low computational costs and less training time compared to other methods like BERT and deep neural networks.
